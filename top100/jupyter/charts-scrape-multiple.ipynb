{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datetime\n",
    "\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-a531a928b24c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m#df['month'] = datetime.datetime.strptime(weekdates[0], \"%d.%m.%Y\").month\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweekdates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"%d.%m.%Y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mcw\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0msavename_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../pickle/charts_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".pkl\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "url = \"https://www.offiziellecharts.de/charts/single/for-date-1515106800000\"\n",
    "\n",
    "cw = 0\n",
    "y = 2018\n",
    "\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    rank_current = []\n",
    "    artist = []\n",
    "    title = []\n",
    "    label = []\n",
    "    incharts = []\n",
    "    peak = []\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    soup = bs(page.content, parser=\"html5lib\")\n",
    "    \n",
    "    #finding out the start and end date of the top 100 songs list\n",
    "    periodhelp = soup.find_all('strong')\n",
    "    \n",
    "    weekdates = []\n",
    "    for dt in periodhelp:\n",
    "        weekdates.append(dt.text)\n",
    "    #getting list of current rank\n",
    "    \n",
    "    rankc_help = soup.find_all('span', {'class' : 'this-week'})\n",
    "    rank_current = [int(r.text) for r in rankc_help]\n",
    "    \n",
    "    #getting list of artists\n",
    "    artist_help = soup.find_all('span', {'class' : 'info-artist'})\n",
    "    artist = [r.text for r in artist_help]\n",
    "    \n",
    "    #getting list of titles\n",
    "    title_help = soup.find_all('span', {'class' : 'info-title'})\n",
    "    title = [r.text for r in title_help]\n",
    "    \n",
    "    #getting list of labels\n",
    "    label_help = soup.find_all('span', {'class' : 'info-label'})\n",
    "    label = [r.text for r in label_help]\n",
    "    \n",
    "    #getting list of in charts information\n",
    "    \n",
    "    incharts_help = soup.find_all('span', {'class' : 'plus-data'})\n",
    "    incharts = [r.text.split()[2] for r in incharts_help if len(r.text.split()) > 2]\n",
    "    \n",
    "    #getting list of overall peak position information\n",
    "    peak_help = soup.find_all('span', {'class' : 'plus-data'})\n",
    "    peak = [r.text.split()[1] for r in peak_help if len(r.text.split()) < 3]\n",
    "    \n",
    "    dict_help = {'current_rank': rank_current,\n",
    "                 'artist': artist,\n",
    "                 'title': title,\n",
    "                 'label': label,\n",
    "                 'in_charts': incharts,\n",
    "                 'peak': peak,\n",
    "                }\n",
    "    \n",
    "    df = pd.DataFrame(dict_help)\n",
    "    #df['weekbegin'] = datetime.datetime.strptime(weekdates[0], \"%d.%m.%Y\")\n",
    "    #df['weekend'] = datetime.datetime.strptime(weekdates[1], \"%d.%m.%Y\")\n",
    "    #df['year'] = datetime.datetime.strptime(weekdates[0], \"%d.%m.%Y\").year\n",
    "    #df['month'] = datetime.datetime.strptime(weekdates[0], \"%d.%m.%Y\").month\n",
    "    \n",
    "    if int(datetime.datetime.strptime(weekdates[0], \"%d.%m.%Y\").year) == y:\n",
    "        cw += 1\n",
    "        savename_pickle = \"../pickle/charts_\" + str(y) + \"_\" + str(cw) + \".pkl\"\n",
    "        savename_csv = \"../csv/charts_\" + str(y) + \"_\" + str(cw) + \".csv\"\n",
    "        df.to_pickle(savename_pickle)\n",
    "        df.to_csv(savename_csv, index=False)\n",
    "        \n",
    "    elif int(datetime.datetime.strptime(weekdates[0], \"%d.%m.%Y\").year) > y:\n",
    "        y == int(datetime.datetime.strptime(weekdates[0], \"%d.%m.%Y\").year)\n",
    "        cw = 1\n",
    "        savename_pickle = \"../pickle/charts_\" + str(y) + \"_\" + str(cw) + \".pkl\"\n",
    "        savename_csv = \"../csv/charts_\" + str(y) + \"_\" + str(cw) + \".csv\"\n",
    "        df.to_pickle(savename_pickle)\n",
    "        df.to_csv(savename_csv, index=False)\n",
    "    \n",
    "    if len(soup.find_all('a', {'class' : 'next-link btn btn-default btn-sm'})) > 0:\n",
    "        url = \"https://www.offiziellecharts.de/charts/single\" + [a['href'] for a in soup.find_all('a', {'class' : 'next-link btn btn-default btn-sm'})][0]\n",
    "        #Nap time!\n",
    "        wait_time = randint(1,5)\n",
    "        sleep(wait_time)\n",
    "        continue\n",
    "    \n",
    "    elif (len(soup.find_all('a', {'class' : 'next-link btn btn-default btn-sm'})) <= 0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.offiziellecharts.de/charts/single/for-date-1515106800000\"\n",
    "\n",
    "rank_current = []\n",
    "artist = []\n",
    "title = []\n",
    "label = []\n",
    "incharts = []\n",
    "peak = []\n",
    "\n",
    "page = requests.get(url)\n",
    "soup = bs(page.content, parser=\"html5lib\")\n",
    "\n",
    "#finding out the start and end date of the top 100 songs list\n",
    "periodhelp = soup.find_all('strong')\n",
    "\n",
    "weekdates = []\n",
    "for date in periodhelp:\n",
    "    weekdates.append(date.text)\n",
    "\n",
    "int(datetime.datetime.strptime(weekdates[0], \"%d.%m.%Y\").year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
